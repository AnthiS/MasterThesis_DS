{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerds.evaluate.score import annotation_precision_recall_f1score\n",
    "from nerds.doc import Document, AnnotatedDocument\n",
    "from nerds.dataset.convert import transform_bio_tags_to_annotated_document\n",
    "\n",
    "def seq_precision_recall_f1score(preds, labels):\n",
    "    if len(preds) != len(labels):\n",
    "        raise ValueError(\"Predicted and valid tags don't match!\")\n",
    "    tokens = [\"TOKEN\" for _ in range(len(preds))]\n",
    "    content = \" \".join(tokens)\n",
    "    document = Document(content=content.encode(\"utf-8\"))\n",
    "    ann_document_pred = transform_bio_tags_to_annotated_document(tokens, preds, document)\n",
    "    ann_document_true = transform_bio_tags_to_annotated_document(tokens, labels, document)\n",
    "    return annotation_precision_recall_f1score([ann_document_pred], [ann_document_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/symeonidoua/Desktop/Elsevier_bert_results_entity-level.txt', sep=\" \", header=None)\n",
    "data.columns = [\"tokens_true\", \"labels\", \"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = data['labels'].replace({'B-ADR' :'B_Adverse_drug_reaction', 'I-ADR' : 'I_Adverse_drug_reaction', 'O-ADR' :'O'})\n",
    "data['preds'] = data['preds'].replace({'B-Adverse_drug_reaction' :'B', 'I-Adverse_drug_reaction' : 'I', 'O-ADR' :'O'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(data['preds'])\n",
    "labels = list(data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9985835694050992, 0.9992912827781716)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_precision_recall_f1score(predictions, labels)\n",
    "# 0.7444852941176471, 0.7548928238583411, 0.7496529384544193 Elsevier results with bert predictions using "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerds",
   "language": "python",
   "name": "nerds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
